{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "q_AWQ0-U1DYt"
      },
      "outputs": [],
      "source": [
        "# general imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import itertools\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe of tweets from LA Times Health and ignore the id and timestamp columns\n",
        "tweets_df = pd.read_csv('https://raw.githubusercontent.com/nikhilmanda9/Tweets-Similarity/main/latimeshealth.txt', delimiter='|', names=('id', 'timestamp', 'tweet'), usecols=['tweet'],header=None)\n",
        "\n",
        "# convert the dataframe to a nested list of tweets\n",
        "tweets = tweets_df.values.tolist()"
      ],
      "metadata": {
        "id": "KtVai-fnrEun"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to perform data preprocessing\n",
        "def preprocess_tweets(tweets):\n",
        "\n",
        "  # regular expression to extract URL\n",
        "  url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "\n",
        "  # regular expression to extract '@' pattern\n",
        "  mention_pattern = re.compile(r'@(\\w+)')\n",
        "\n",
        "  # regular expression to extract '#' pattern\n",
        "  hashtag_pattern = re.compile(r'#(\\w+)')\n",
        "\n",
        "\n",
        "  for k in range(len(tweets)):\n",
        "    # remove the URL from the tweet\n",
        "    tweets[k][0] = url_pattern.sub('', tweets[k][0])\n",
        "\n",
        "    # remove the '@' character from the tweet\n",
        "    tweets[k][0] = mention_pattern.sub(r'\\1', tweets[k][0])\n",
        "\n",
        "    # remove the '#' character from the tweet\n",
        "    tweets[k][0] = hashtag_pattern.sub(r'\\1', tweets[k][0])\n",
        "\n",
        "    # remove trailing whitespace at the end of the tweet\n",
        "    tweets[k][0] = tweets[k][0].strip()\n",
        "\n",
        "    # ensure all words in the tweet are lowercase\n",
        "    tweets[k][0] = tweets[k][0].lower()"
      ],
      "metadata": {
        "id": "jPk4tExCq1zl"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function computes the jaccard distance metric\n",
        "def jaccard_distance(a,b):\n",
        "  intersection = len(a.intersection(b))\n",
        "  union = len(a.union(b))\n",
        "  return 1 - (intersection / union)"
      ],
      "metadata": {
        "id": "ONQR9r6tHfGS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function used to update and recompute centroid for a cluster\n",
        "def compute_centroid(cluster):\n",
        "  # Compute centroid as the tweet with the minimum distance to all other tweets in same cluster\n",
        "  min_distances = [float('inf')] * len(cluster)\n",
        "  centroid_index = 0\n",
        "\n",
        "  for k, tweet in enumerate(cluster):\n",
        "    distance_sum = sum(jaccard_distance(tweet, other) for j, other in enumerate(cluster) if k != j)\n",
        "    if distance_sum < min_distances[k]:\n",
        "      min_distances[k] = distance_sum\n",
        "      centroid_index = k\n",
        "\n",
        "  return cluster[centroid_index]"
      ],
      "metadata": {
        "id": "MJeUhd3FJ9Y8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to perform the k means clustering algorithm\n",
        "def k_means_clustering(tweets, k):\n",
        "  # Initialize centroids randomly\n",
        "  np.random.shuffle(tweets)\n",
        "  centroids = tweets[:k]\n",
        "\n",
        "  # Initialize clusters\n",
        "  clusters = [[] for _ in range(k)]\n",
        "\n",
        "  # repeat until convergence\n",
        "  while True:\n",
        "    # Assign tweets to the nearest cluster\n",
        "    new_clusters = [[] for _ in range(k)]\n",
        "    for tweet in tweets:\n",
        "      distances = [jaccard_distance(tweet, centroid) for centroid in centroids]\n",
        "      nearest_cluster = np.argmin(distances)\n",
        "      new_clusters[nearest_cluster].append(tweet)\n",
        "\n",
        "    # check for convergence\n",
        "    if new_clusters == clusters:\n",
        "      break\n",
        "\n",
        "    # Update centroids\n",
        "    clusters = new_clusters\n",
        "    for i in range(k):\n",
        "      centroids[i] = compute_centroid(clusters[i])\n",
        "\n",
        "  return clusters"
      ],
      "metadata": {
        "id": "zXna5IqkLdMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sse(clusters):\n",
        "  sse = 0\n",
        "  for j, cluster in enumerate(clusters):\n",
        "    centroid = compute_centroid(cluster)\n",
        "    sse += sum(jaccard_distance(tweet, centroid) ** 2 for tweet in cluster)\n",
        "  return sse"
      ],
      "metadata": {
        "id": "BaOE_qUDOFvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the tweets data\n",
        "preprocess_tweets(tweets)\n",
        "\n",
        "# convert tweets to sets in order to compute jaccard distance\n",
        "tweet_sets = [set(tweet[0].split()) for tweet in tweets]\n",
        "\n",
        "k_values = [2,4,6,8,10]\n",
        "\n",
        "results = [(compute_sse(k_means_clustering(tweet_sets, k)), [len(cluster) for cluster in k_means_clustering(tweet_sets, k)]) for k in k_values]\n",
        "\n",
        "with open('k_means_logs.txt', \"a\") as logfile:\n",
        "  for k, (sse, cluster_sizes) in zip(k_values, results):\n",
        "    for i, size in enumerate(cluster_sizes):\n",
        "      logfile.write(f\"k = {k}\\n\")\n",
        "      logfile.write(f\"SSE: {sse}\\n\")\n",
        "      for i, size in enumerate(cluster_sizes):\n",
        "        logfile.write(f\"Cluster {i + 1} Size: {size}\\n\")\n",
        "      logfile.write('\\n')\n",
        "\n",
        "  values = [[k, sse, {f\"Cluster {i + 1}\": len(cluster) for i,\n",
        "                      cluster in enumerate(k_means_clustering(tweet_sets, k))}]\n",
        "            for k, (sse, _) in zip(k_values, results)]\n",
        "\n",
        "  logfile.write(tabulate(values, headers=['k', 'SSE', 'Size of each cluster']))"
      ],
      "metadata": {
        "id": "RoMOnl6uPj_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}