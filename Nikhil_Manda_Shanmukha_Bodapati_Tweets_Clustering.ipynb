{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q_AWQ0-U1DYt"
      },
      "outputs": [],
      "source": [
        "# General Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import itertools\n",
        "from tabulate import tabulate\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of tweets from the Fox News Health dataset\n",
        "# Ignore the tweet id and timestamp columns\n",
        "tweets_df = pd.read_csv('https://raw.githubusercontent.com/nikhilmanda9/Tweets-Similarity/main/foxnewshealth.txt',\n",
        "                        delimiter='|', names=('id', 'timestamp', 'tweet'),\n",
        "                        usecols=['tweet'], header=None, encoding='cp1252')\n",
        "print(tweets_df.head())\n",
        "\n",
        "# Convert the dataframe to a list of tweets\n",
        "tweets = list(tweets_df['tweet'])"
      ],
      "metadata": {
        "id": "KtVai-fnrEun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0add230e-233f-4d8f-9952-c06070d8ced1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet\n",
            "0  Injury prevention programs unpopular with high...\n",
            "1  6 dietary changes to make midlife http://ow.ly...\n",
            "2  Massachusetts governor gets head shaved to sup...\n",
            "3  Dad wins 3 marathons in 8 days; winnings to he...\n",
            "4     Possible cure for melanoma? http://ow.ly/LlLg8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform data preprocessing\n",
        "def preprocess_tweets(tweets):\n",
        "  # Remove any word that starts with the symbol '@' using regular expression\n",
        "  mention_pattern = re.compile(r'@(\\w+)')\n",
        "  tweets = [mention_pattern.sub(r'\\1', tweet) for tweet in tweets]\n",
        "\n",
        "  # Remove any hashtag symbols using regular expression\n",
        "  hashtag_pattern = re.compile(r'#(\\w+)')\n",
        "  tweets = [hashtag_pattern.sub(r'\\1', tweet) for tweet in tweets]\n",
        "\n",
        "  # Remove the URL from the tweet using regular expression\n",
        "  url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "  tweets = [url_pattern.sub('', tweet) for tweet in tweets]\n",
        "\n",
        "  # Ensure all words in the tweet are lowercase\n",
        "  tweets = [tweet.lower() for tweet in tweets]"
      ],
      "metadata": {
        "id": "jPk4tExCq1zl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the Jaccard distance assuming each tweet is\n",
        "# considered as an unordered set of words\n",
        "def jaccard_distance(a, b):\n",
        "  intersection = float(len(a.intersection(b)))\n",
        "  union = float(len(a.union(b)))\n",
        "  return 1 - (intersection / union)"
      ],
      "metadata": {
        "id": "ONQR9r6tHfGS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to update centroid for a cluster\n",
        "# The new centroid will be the tweet with the minimum sum squared distance to all\n",
        "# other tweets in the same cluster. More specifically, find the tweet whose\n",
        "# squared distance to all other tweets in the cluster summed is the smallest.\n",
        "def update_centroids(cluster):\n",
        "  # Find the distance matrix to make calculations faster\n",
        "  distance_matrix = []\n",
        "  for i in range(len(cluster)):\n",
        "      distance_matrix.append([jaccard_distance(cluster[i], cluster[j]) ** 2 for j in range(len(cluster))])\n",
        "\n",
        "  distances = [sum(distance_matrix[i]) ** 2 for i in range(len(cluster))]\n",
        "  centroid_index = np.argmin(distances)\n",
        "\n",
        "  return cluster[centroid_index]"
      ],
      "metadata": {
        "id": "MJeUhd3FJ9Y8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform the k-means clustering algorithm\n",
        "def k_means_clustering(tweets, k):\n",
        "  # Initialize centroids randomly by shuffling the tweets and selecting the\n",
        "  # first k tweets as cluster centroids\n",
        "  np.random.shuffle(tweets)\n",
        "  centroids = tweets[:k]\n",
        "\n",
        "  # Initialize clusters\n",
        "  clusters = [[] for _ in range(k)]\n",
        "  test = 0\n",
        "  # Repeat until convergence (when clusters no longer change)\n",
        "  while True:\n",
        "    new_clusters = [[] for _ in range(k)]\n",
        "\n",
        "    # Assign tweets to the nearest cluster\n",
        "    # This is done by finding the cluster whose distance from the tweet\n",
        "    # is the smallest and assigning the tweet to that cluster\n",
        "    for tweet in tweets:\n",
        "      cluster_distances = [jaccard_distance(tweet, centroid) for centroid in centroids]\n",
        "      nearest_cluster = np.argmin(cluster_distances)\n",
        "      new_clusters[nearest_cluster].append(tweet)\n",
        "\n",
        "    # Check for convergence (if the new clusters are the same as the previous)\n",
        "    if new_clusters == clusters:\n",
        "      return clusters, centroids\n",
        "\n",
        "    # Update centroids\n",
        "    clusters = new_clusters\n",
        "    for i in range(k):\n",
        "      centroids[i] = update_centroids(clusters[i])\n",
        "    test += 1"
      ],
      "metadata": {
        "id": "zXna5IqkLdMi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the Sum of Squared Error as the sum of squared distances\n",
        "# of every point in every cluster to its cluster's centroid\n",
        "def compute_sse(clusters, centroids):\n",
        "  sse = 0\n",
        "  for i in range(len(centroids)):\n",
        "    sse += sum([jaccard_distance(x, centroids[i]) ** 2 for x in clusters[i]])\n",
        "  return sse"
      ],
      "metadata": {
        "id": "BaOE_qUDOFvW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tweets data\n",
        "preprocess_tweets(tweets)\n",
        "\n",
        "# Convert tweets to sets in order to compute jaccard distance\n",
        "tweets = [set(tweet.split()) for tweet in tweets]\n",
        "\n",
        "# Test with 5 different values of K\n",
        "k_values = [5, 10, 15, 20, 25]\n",
        "\n",
        "# Create a log file and find the results of the k-means algorithm\n",
        "# with each value of k given above\n",
        "with open('k_means_logs.txt', \"a\") as logfile:\n",
        "  # Store the k-value and resulting clusters after running the algorithm\n",
        "  values = []\n",
        "\n",
        "  for k in k_values:\n",
        "    clusters, centroids = k_means_clustering(tweets, k)\n",
        "\n",
        "    # Get the SSE\n",
        "    sse = compute_sse(clusters, centroids)\n",
        "\n",
        "    # Get the size of each cluster\n",
        "    sizes = defaultdict(int)\n",
        "    for i in range(len(clusters)):\n",
        "      sizes[i+1] = len(clusters[i])\n",
        "\n",
        "    check = True\n",
        "    for x in sizes:\n",
        "      if check:\n",
        "        values.append([k, sse, \"{}: {}\".format(x, sizes[x])])\n",
        "        check = False\n",
        "      else:\n",
        "        values.append([\"\", \"\", \"{}: {}\".format(x, sizes[x])])\n",
        "\n",
        "  # Write the results to the logfile\n",
        "  logfile.write(tabulate(values,\n",
        "                headers=['Value of K', 'SSE', 'Size of each cluster']))"
      ],
      "metadata": {
        "id": "RoMOnl6uPj_c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of tweets from the US News Health dataset\n",
        "# Ignore the tweet id and timestamp columns\n",
        "tweets_df = pd.read_csv('https://raw.githubusercontent.com/nikhilmanda9/Tweets-Similarity/main/usnewshealth.txt',\n",
        "                        delimiter='|', names=('id', 'timestamp', 'tweet'),\n",
        "                        usecols=['tweet'], header=None, encoding='utf-8')\n",
        "print(tweets_df.head())\n",
        "\n",
        "# Convert the dataframe to a list of tweets\n",
        "tweets = list(tweets_df['tweet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibhVkzZPpMMx",
        "outputId": "280b5369-72dc-4139-cc65-ad7b6e6401f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet\n",
            "0  Planning to hire a personal trainer? Read thes...\n",
            "1  RT @AnnaMedaris: Any dads out their who strugg...\n",
            "2  America's problem with diabetes in one map: ht...\n",
            "3  Think water &amp; fiber will cure your constip...\n",
            "4  About to lose it? Here, try one of these offic...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tweets data\n",
        "preprocess_tweets(tweets)\n",
        "\n",
        "# Convert tweets to sets in order to compute jaccard distance\n",
        "tweets = [set(tweet.split()) for tweet in tweets]\n",
        "\n",
        "# Test with 5 different values of K\n",
        "k_values = [5, 10, 15, 20, 25]\n",
        "\n",
        "# Create a log file and find the results of the k-means algorithm\n",
        "# with each value of k given above\n",
        "with open('k_means_logs1.txt', \"a\") as logfile:\n",
        "  # Store the k-value and resulting clusters after running the algorithm\n",
        "  values = []\n",
        "\n",
        "  for k in k_values:\n",
        "    clusters, centroids = k_means_clustering(tweets, k)\n",
        "\n",
        "    # Get the SSE\n",
        "    sse = compute_sse(clusters, centroids)\n",
        "\n",
        "    # Get the size of each cluster\n",
        "    sizes = defaultdict(int)\n",
        "    for i in range(len(clusters)):\n",
        "      sizes[i+1] = len(clusters[i])\n",
        "\n",
        "    check = True\n",
        "    for x in sizes:\n",
        "      if check:\n",
        "        values.append([k, sse, \"{}: {}\".format(x, sizes[x])])\n",
        "        check = False\n",
        "      else:\n",
        "        values.append([\"\", \"\", \"{}: {}\".format(x, sizes[x])])\n",
        "\n",
        "  # Write the results to the logfile\n",
        "  logfile.write(tabulate(values,\n",
        "                headers=['Value of K', 'SSE', 'Size of each cluster']))"
      ],
      "metadata": {
        "id": "VORboe_9tRXp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of tweets from the LA Times Health dataset\n",
        "# Ignore the tweet id and timestamp columns\n",
        "tweets_df = pd.read_csv('https://raw.githubusercontent.com/nikhilmanda9/Tweets-Similarity/main/latimeshealth.txt',\n",
        "                        delimiter='|', names=('id', 'timestamp', 'tweet'),\n",
        "                        usecols=['tweet'], header=None, encoding='utf-8')\n",
        "print(tweets_df.head())\n",
        "\n",
        "# Convert the dataframe to a list of tweets\n",
        "tweets = list(tweets_df['tweet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj_BM9g4tuG-",
        "outputId": "ba73e32c-11b9-4955-d6ad-d1da63184b84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet\n",
            "0  Five new running shoes that aim to go the extr...\n",
            "1  Gym Rat: Disq class at Crunch is intense worko...\n",
            "2  Noshing through thousands of ideas at Natural ...\n",
            "3  Natural Products Expo also explores beauty, su...\n",
            "4  Free Fitness Weekends in South Bay beach citie...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tweets data\n",
        "preprocess_tweets(tweets)\n",
        "\n",
        "# Convert tweets to sets in order to compute jaccard distance\n",
        "tweets = [set(tweet.split()) for tweet in tweets]\n",
        "\n",
        "# Test with 5 different values of K\n",
        "k_values = [5, 10, 15, 20, 25]\n",
        "\n",
        "# Create a log file and find the results of the k-means algorithm\n",
        "# with each value of k given above\n",
        "with open('k_means_logs2.txt', \"a\") as logfile:\n",
        "  # Store the k-value and resulting clusters after running the algorithm\n",
        "  values = []\n",
        "\n",
        "  for k in k_values:\n",
        "    clusters, centroids = k_means_clustering(tweets, k)\n",
        "\n",
        "    # Get the SSE\n",
        "    sse = compute_sse(clusters, centroids)\n",
        "\n",
        "    # Get the size of each cluster\n",
        "    sizes = defaultdict(int)\n",
        "    for i in range(len(clusters)):\n",
        "      sizes[i+1] = len(clusters[i])\n",
        "\n",
        "    check = True\n",
        "    for x in sizes:\n",
        "      if check:\n",
        "        values.append([k, sse, \"{}: {}\".format(x, sizes[x])])\n",
        "        check = False\n",
        "      else:\n",
        "        values.append([\"\", \"\", \"{}: {}\".format(x, sizes[x])])\n",
        "\n",
        "  # Write the results to the logfile\n",
        "  logfile.write(tabulate(values,\n",
        "                headers=['Value of K', 'SSE', 'Size of each cluster']))"
      ],
      "metadata": {
        "id": "mj11jm1_t7nI"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}